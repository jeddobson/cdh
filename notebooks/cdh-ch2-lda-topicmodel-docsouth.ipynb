{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining DocSouth Slave Narrative Archive\n",
    "---\n",
    "\n",
    "*Note:* This is one in [a series of documents and notebooks](https://jeddobson.github.io/textmining-docsouth/) that will document and evaluate various machine learning and text mining tools for use in literary studies. These notebooks form the practical and critical archive of my book-in-progress, _Digital Humanities and the Search for a Method_. I have published a critique of some existing methods (Dobson 2016) that takes up some of these concerns and provides some theoretical background for my account of computational methods as used within the humanities.\n",
    "\n",
    "### Revision Date and Notes:\n",
    "\n",
    "10/12/2017: Initial version (james.e.dobson@dartmouth.edu)\n",
    "\n",
    "### Producing Topic Models from DocSouth North American Slave Narrative Texts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# local Natural Language Toolkit\n",
    "import nltk\n",
    "print(\"nltk version: \",nltk.__version__)\n",
    "\n",
    "# load scikit-learn \n",
    "import sklearn\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn import decomposition\n",
    "from sklearn import datasets\n",
    "print(\"sklearn version: \",sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load all library and all the texts\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"lib\")\n",
    "import docsouth_utils\n",
    "\n",
    "neh_slave_archive = docsouth_utils.load_narratives()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# create input list of documents for the topic model\n",
    "# and perform additional preprocessing (stopword removal, \n",
    "#  lowercase, dropping non-alpha characters, etc.)\n",
    "#\n",
    "\n",
    "topic_model_source=list()\n",
    "for i in neh_slave_archive:\n",
    "    preprocessed=docsouth_utils.preprocess(i['text'])\n",
    "    topic_model_source.append(' '.join(preprocessed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# topics to model\n",
    "num_topics = 20\n",
    "\n",
    "# features to extract\n",
    "num_features = 50\n",
    "\n",
    "print('reading files and loading into vectorizer')\n",
    "print('generating',num_topics,'topics with',num_features,'features')\n",
    "\n",
    "# for LDA (Just TF)\n",
    "lda_vectorizer = text.CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=num_features,\n",
    "                                lowercase='true',\n",
    "                                ngram_range=(2,4),\n",
    "                                strip_accents='unicode',\n",
    "                                stop_words='english')\n",
    "\n",
    "lda_vectorizer.decode_error='replace'\n",
    "lda_tf = lda_vectorizer.fit_transform(topic_model_source)\n",
    "\n",
    "# fit to model\n",
    "lda_model = decomposition.LatentDirichletAllocation(n_topics=num_topics, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                batch_size=128,\n",
    "                                max_doc_update_iter=100,\n",
    "                                random_state=None)\n",
    "lda_model.fit(lda_tf)\n",
    "\n",
    "print(\"LDA Model:\")\n",
    "feature_names = lda_vectorizer.get_feature_names()\n",
    "for topic_idx, topic in enumerate(lda_model.components_):\n",
    "    print(\"Topic #%d:\" % topic_idx)\n",
    "    print(\", \".join([feature_names[i] for i in topic.argsort()[:-num_features - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
